{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb28e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77464844",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlindataloader02-multi-gpu-tensorflow-with-horovod/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Multi-GPU training with Tensorflow and Horovod\n",
    "\n",
    "This notebook is created using the latest stable [merlin-tensorflow](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow) container.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook we will look at multi-GPU training with Tensorflow and Horovod. [Horovod](https://horovod.ai/) is a distributed deep learning framework that aims to make distributed deep learning fast and easy to use.\n",
    "\n",
    "In this example, we will provide a simple pipeline to train a MatrixFactorization Model in TensorFlow on multiple GPUs (for the example we will use two but this method can be easily extended to use more).\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "- Training on multiple GPUs with Merlind Dataloader and Horovod."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5598ae",
   "metadata": {},
   "source": [
    "# Downloading and preparing the dataset\n",
    "\n",
    "We will base our example on the  [MovieLens25M](https://grouplens.org/datasets/movielens/25m/) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd46306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from merlin.core.utils import download_file\n",
    "from merlin.core.dispatch import get_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591f8c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.environ.get(\"DATA_PATH\", os.path.expanduser(\"~/workspace\"))\n",
    "download_file(\"http://files.grouplens.org/datasets/movielens/ml-25m.zip\", DATA_PATH + \"/ml-25m.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc70934",
   "metadata": {},
   "source": [
    "# Training a TensorFlow Keras Model with Merlin dataloader and Horovod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b46668",
   "metadata": {},
   "source": [
    "This example can be scaled to running on as many GPUs as you would like.\n",
    "\n",
    "In this example, we will implement data parallel training. Each GPU will have an exact copy of our model, however it will train on different subsets of data.\n",
    "\n",
    "Let's us split our train data into as many parquet files as are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e5ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_COUNT = 2  # specify how many GPUs you would like to train on\n",
    "\n",
    "ratings = get_lib().read_csv(DATA_PATH + \"/ml-25m/ratings.csv\")\n",
    "\n",
    "ratings.to_parquet(os.path.join(DATA_PATH, \"train.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9a65b4",
   "metadata": {},
   "source": [
    "Let us now take a closer look at what else we will need to train with Horovod.\n",
    "\n",
    "### Write the training script to a file\n",
    "\n",
    "We need to have a `.py` file we will be able to load into each process using `horovodrun`.\n",
    "\n",
    "### Set `CUDA visible devices` correctly inside each process\n",
    "\n",
    "We need to set the visible device in each process to its `rank`. This way process with `rank 0` will use the zeroth GPU, process with `rank 1` will use the first GPU, and so on. This ensures that each worker can access only a single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbe17a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"./tf_trainer.py\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# the order of statements and imports is imoportant\n",
    "# for instance, we need to make sure we set\n",
    "# CUDA_VISIBLE_DEVICES before we import Loader and cudf\n",
    "\n",
    "MPI_SIZE = int(os.getenv(\"OMPI_COMM_WORLD_SIZE\"))\n",
    "MPI_RANK = int(os.getenv(\"OMPI_COMM_WORLD_RANK\"))\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(MPI_RANK)\n",
    "\n",
    "\n",
    "from merlin.io import Dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "import horovod.tensorflow as hvd\n",
    "\n",
    "from merlin.core.dispatch import get_lib\n",
    "\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "hvd.init()\n",
    "\n",
    "from merlin.loader.tensorflow import Loader\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data_path\", default=None, help=\"Input directory.\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=None, help=\"Batch size.\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "DATA_PATH = args.data_path or os.path.expanduser(\"~/workspace\")\n",
    "BATCH_SIZE = args.batch_size or 1024\n",
    "\n",
    "dataset = Dataset(os.path.join(DATA_PATH, \"train.parquet\"))\n",
    "dataset = dataset.repartition(MPI_SIZE)\n",
    "\n",
    "loader = Loader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    global_size=MPI_SIZE,\n",
    "    global_rank=MPI_RANK,\n",
    "    device=MPI_RANK,\n",
    ")\n",
    "\n",
    "label_column = 'rating'\n",
    "\n",
    "\n",
    "def process_batch(data, _):\n",
    "    x = {col: data[col] for col in data.keys() if col != label_column}\n",
    "    y = data[label_column]\n",
    "    return (x, y)\n",
    "\n",
    "\n",
    "loader._map_fns = [process_batch]\n",
    "\n",
    "\n",
    "class MatrixFactorization(tf.keras.Model):\n",
    "    def __init__(self, n_factors):\n",
    "        super().__init__()\n",
    "        self.user_embeddings = tf.keras.layers.Embedding(162542, n_factors)\n",
    "        self.movie_embeddings = tf.keras.layers.Embedding(209172, n_factors)\n",
    "\n",
    "    def call(self, batch, training=False):\n",
    "        user_embs = self.user_embeddings(batch['userId'])\n",
    "        movie_embs = self.movie_embeddings(batch['movieId'])\n",
    "\n",
    "        tensor = (tf.squeeze(user_embs) * tf.squeeze(movie_embs))\n",
    "        return tf.reduce_sum(tensor, 1)\n",
    "\n",
    "\n",
    "model = MatrixFactorization(64)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "opt = tf.optimizers.Adam(1e-2 * hvd.size())\n",
    "\n",
    "checkpoint_prefix = \"./checkpoints\"\n",
    "checkpoint = tf.train.Checkpoint(model=model, optimizer=opt)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def training_step(features, labels, first_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        probs = model(features, training=True)\n",
    "        loss_value = loss(labels, probs)\n",
    "\n",
    "    # Horovod: add Horovod Distributed GradientTape.\n",
    "    tape = hvd.DistributedGradientTape(tape)\n",
    "\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
    "    # This is necessary to ensure consistent initialization of all workers when\n",
    "    # training is started with random weights or restored from a checkpoint.\n",
    "    #\n",
    "    # Note: broadcast should be done after the first gradient step to ensure optimizer\n",
    "    # initialization.\n",
    "    if first_batch:\n",
    "        hvd.broadcast_variables(model.variables, root_rank=0)\n",
    "        hvd.broadcast_variables(opt.variables(), root_rank=0)\n",
    "\n",
    "    return loss_value\n",
    "\n",
    "\n",
    "# Horovod: adjust number of steps based on number of GPUs.\n",
    "for batch, (features, labels) in enumerate(loader):\n",
    "    loss_value = training_step(features, labels, batch == 0)\n",
    "\n",
    "    if batch % 10 == 0 and hvd.rank() == 0:\n",
    "        print('Step #%d\\tLoss: %.6f' % (batch, loss_value))\n",
    "\n",
    "hvd.join()\n",
    "\n",
    "# Horovod: save checkpoints only on worker 0 to prevent other workers from\n",
    "# corrupting it.\n",
    "if hvd.rank() == 0:\n",
    "    checkpoint.save(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ca4b5",
   "metadata": {},
   "source": [
    "We now can run our distributed training using `horovodrun`!\n",
    "\n",
    "All we need to do is provide the number of GPUs we would like to run on and the script to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5e9b7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! horovodrun -np {GPU_COUNT} python tf_trainer.py --data_path={DATA_PATH} --batch_size=65536"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d97fe2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We demonstrated how to train a TensorFlow Keras model with the Merlin dataloader on multiple GPUs using Horovod.\n",
    "\n",
    "# Next Steps\n",
    "\n",
    "Merlin dataloader is part of NVIDIA Merlin, a open source framework for recommender systems. In this example, we looked only on a specific use-case to accelerate existing training pipelines. We provide more libraries to make recommender system pipelines easier and faster to work with:\n",
    "\n",
    "* [NVTabular](https://github.com/NVIDIA-Merlin/NVTabular) is a library to accelerate and scale feature engineering\n",
    "* [Merlin Models](https://github.com/NVIDIA-Merlin/models) is a library with high-quality implementations of popular recommender systems architectures\n",
    "\n",
    "The libraries are designed to work closely together. We recommend to check out our examples:\n",
    "\n",
    "* [Getting Started with NVTabular: Process Tabular Data On GPU](https://github.com/NVIDIA-Merlin/NVTabular/blob/stable/examples/01-Getting-started.ipynb)\n",
    "* [Getting Started with Merlin Models: Develop a Model for MovieLens](https://github.com/NVIDIA-Merlin/models/blob/stable/examples/01-Getting-started.ipynb)\n",
    "\n",
    "In the example, [From ETL to Training RecSys models - NVTabular and Merlin Models integrated example](https://github.com/NVIDIA-Merlin/models/blob/stable/examples/02-Merlin-Models-and-NVTabular-integration.ipynb), we explain how the close collaboration works."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
