{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb28e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77464844",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlindataloader02-multi-gpu-tensorflow-with-horovod/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Multi-GPU training with Tensorflow and Horovod\n",
    "\n",
    "This notebook is created using the latest stable [merlin-tensorflow](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow) container.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook we will look at multi-GPU training with Tensorflow and Horovod. [Horovod](https://horovod.ai/) is a distributed deep learning framework that aims to make distributed deep learning fast and easy to use.\n",
    "\n",
    "In this example, we will provide a simple pipeline to train a MatrixFactorization Model in TensorFlow on multiple GPUs (for the example we will use two but this method can be easily extended to use more).\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "- Training on multiple GPUs with Merlind Dataloader and Horovod."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5598ae",
   "metadata": {},
   "source": [
    "# Downloading and preparing the dataset\n",
    "\n",
    "We will base our example on the  [MovieLens25M](https://grouplens.org/datasets/movielens/25m/) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edd46306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.core.utils import download_file\n",
    "from merlin.core.dispatch import get_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591f8c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "downloading ml-25m.zip: 262MB [00:07, 36.0MB/s]                            \n",
      "unzipping files: 100%|██████████| 8/8 [00:08<00:00,  1.08s/files]\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = '/workspace'\n",
    "download_file(\"http://files.grouplens.org/datasets/movielens/ml-25m.zip\", DATA_PATH + \"/ml-25m.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc70934",
   "metadata": {},
   "source": [
    "# Training a TensorFlow Keras Model with Merlin dataloader and Horovod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b46668",
   "metadata": {},
   "source": [
    "This example can be scaled to running on as many GPUs as you would like.\n",
    "\n",
    "In this example, we will implement data parallel training. Each GPU will have an exact copy of our model, however it will train on different subsets of data.\n",
    "\n",
    "Let's us split our train data into as many parquet files as are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c65e5ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_COUNT = 2  # specify how many GPUs you would like to train on\n",
    "\n",
    "ratings = get_lib().read_csv(DATA_PATH + '/ml-25m/ratings.csv')\n",
    "\n",
    "for i in range(GPU_COUNT):\n",
    "    ratings[\n",
    "        int(i * ratings.shape[0] / GPU_COUNT):int((i + 1) * ratings.shape[0] / GPU_COUNT)\n",
    "    ].to_parquet(DATA_PATH + f'/train_{i}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9a65b4",
   "metadata": {},
   "source": [
    "**Important: Individual parquet files require to have the same number of batches. If one worker has more batches than another, the training process will freeze. At one point during the training process, the worker with more batches waits for the gradients from the worker with fewer batches. But the worker with fewer batches finished the training run.**`\n",
    "\n",
    "Let us now take a closer look at what else we will need to train with Horovod.\n",
    "\n",
    "### Write the training script to a file\n",
    "\n",
    "We need to have a `.py` file we will be able to load into each process using `horovodrun`. \n",
    "\n",
    "### Set `CUDA visible devices` correctly inside each process\n",
    "\n",
    "We need to set the visible device in each process to its `rank`. This way process with `rank 0` will use the zeroth GPU, process with `rank 1` will use the first GPU, and so on. This ensures that each worker can access only a single GPU.\n",
    "\n",
    "Additionally, we will use the `rank` information to select the correct parquet file to load per worker (`DATA_PATH + f'/train_{hvd.local_rank()}.parquet'`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fbe17a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tf_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile './tf_trainer.py'\n",
    "\n",
    "import os\n",
    "\n",
    "# the order of statements and imports is imoportant\n",
    "# for instance, we need to make sure we set\n",
    "# CUDA_VISIBLE_DEVICES before we import Loader and cudf\n",
    "\n",
    "MPI_SIZE = int(os.getenv(\"OMPI_COMM_WORLD_SIZE\"))\n",
    "MPI_RANK = int(os.getenv(\"OMPI_COMM_WORLD_RANK\"))\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(MPI_RANK)\n",
    "\n",
    "\n",
    "from merlin.io import Dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "import horovod.tensorflow.keras as hvd\n",
    "\n",
    "from glob import glob\n",
    "from merlin.core.dispatch import get_lib\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "hvd.init()\n",
    "\n",
    "from merlin.loader.tensorflow import Loader\n",
    "\n",
    "DATA_PATH = '/workspace'\n",
    "\n",
    "dataset = Dataset(glob(DATA_PATH + f'/train_{hvd.local_rank()}.parquet'), part_size=\"100MB\")\n",
    "loader = Loader(dataset, batch_size=64 * 1024)\n",
    "\n",
    "label_column = 'rating'\n",
    "\n",
    "\n",
    "def process_batch(data, _):\n",
    "    x = {col: data[col] for col in data.keys() if col != label_column}\n",
    "    y = data[label_column]\n",
    "    return (x, y)\n",
    "\n",
    "\n",
    "loader._map_fns = [process_batch]\n",
    "\n",
    "\n",
    "class MatrixFactorization(tf.keras.Model):\n",
    "    def __init__(self, n_factors):\n",
    "        super().__init__()\n",
    "        self.user_embeddings = tf.keras.layers.Embedding(162541, n_factors)\n",
    "        self.movie_embeddings = tf.keras.layers.Embedding(209171, n_factors)\n",
    "\n",
    "    def call(self, batch, training=False):\n",
    "        user_embs = self.user_embeddings(batch['userId'])\n",
    "        movie_embs = self.movie_embeddings(batch['movieId'])\n",
    "\n",
    "        tensor = (tf.squeeze(user_embs) * tf.squeeze(movie_embs))\n",
    "        return tf.reduce_sum(tensor, 1)\n",
    "\n",
    "\n",
    "model = MatrixFactorization(64)\n",
    "opt = tf.keras.optimizers.Adam(1e-2 * hvd.size())\n",
    "opt = hvd.DistributedOptimizer(opt)\n",
    "model.compile(optimizer=opt, loss=tf.keras.losses.MeanSquaredError(), experimental_run_tf_function=False)\n",
    "\n",
    "model.fit(\n",
    "    loader,\n",
    "    epochs=1,\n",
    "    callbacks=[hvd.callbacks.BroadcastGlobalVariablesCallback(0)],\n",
    "    verbose=1 if hvd.rank() == 0 else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ca4b5",
   "metadata": {},
   "source": [
    "We now can run our distributed training using `horovodrun`!\n",
    "\n",
    "All we need to do is provide the number of GPUs we would like to run on and the script to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec5e9b7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,0]<stderr>:2022-12-08 06:58:30.501381: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "[1,0]<stderr>:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[1,0]<stderr>:2022-12-08 06:58:30.555187: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0\n",
      "[1,0]<stderr>:2022-12-08 06:58:30.555454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16255 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB-LS, pci bus id: 0000:85:00.0, compute capability: 7.0\n",
      "[1,1]<stderr>:2022-12-08 06:58:30.575717: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "[1,1]<stderr>:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[1,1]<stderr>:2022-12-08 06:58:30.632564: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0\n",
      "[1,1]<stderr>:2022-12-08 06:58:30.632832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16255 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB-LS, pci bus id: 0000:86:00.0, compute capability: 7.0\n",
      "[1,0]<stderr>:2022-12-08 06:58:35.010671: W tensorflow/core/common_runtime/forward_type_inference.cc:231] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "[1,0]<stderr>:type_id: TFT_OPTIONAL\n",
      "[1,0]<stderr>:args {\n",
      "[1,0]<stderr>:  type_id: TFT_PRODUCT\n",
      "[1,0]<stderr>:  args {\n",
      "[1,0]<stderr>:    type_id: TFT_TENSOR\n",
      "[1,0]<stderr>:    args {\n",
      "[1,0]<stderr>:      type_id: TFT_BOOL\n",
      "[1,0]<stderr>:    }\n",
      "[1,0]<stderr>:  }\n",
      "[1,0]<stderr>:}\n",
      "[1,0]<stderr>: is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "[1,0]<stderr>:type_id: TFT_OPTIONAL\n",
      "[1,0]<stderr>:args {\n",
      "[1,0]<stderr>:  type_id: TFT_PRODUCT\n",
      "[1,0]<stderr>:  args {\n",
      "[1,0]<stderr>:    type_id: TFT_TENSOR\n",
      "[1,0]<stderr>:    args {\n",
      "[1,0]<stderr>:      type_id: TFT_LEGACY_VARIANT\n",
      "[1,0]<stderr>:    }\n",
      "[1,0]<stderr>:  }\n",
      "[1,0]<stderr>:}\n",
      "[1,0]<stderr>:\n",
      "[1,0]<stderr>:\twhile inferring type of node 'mean_squared_error/cond/output/_11'\n",
      "[1,1]<stderr>:2022-12-08 06:58:35.218048: W tensorflow/core/common_runtime/forward_type_inference.cc:231] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "[1,1]<stderr>:type_id: TFT_OPTIONAL\n",
      "[1,1]<stderr>:args {\n",
      "[1,1]<stderr>:  type_id: TFT_PRODUCT\n",
      "[1,1]<stderr>:  args {\n",
      "[1,1]<stderr>:    type_id: TFT_TENSOR\n",
      "[1,1]<stderr>:    args {\n",
      "[1,1]<stderr>:      type_id: TFT_BOOL\n",
      "[1,1]<stderr>:    }\n",
      "[1,1]<stderr>:  }\n",
      "[1,1]<stderr>:}\n",
      "[1,1]<stderr>: is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "[1,1]<stderr>:type_id: TFT_OPTIONAL\n",
      "[1,1]<stderr>:args {\n",
      "[1,1]<stderr>:  type_id: TFT_PRODUCT\n",
      "[1,1]<stderr>:  args {\n",
      "[1,1]<stderr>:    type_id: TFT_TENSOR\n",
      "[1,1]<stderr>:    args {\n",
      "[1,1]<stderr>:      type_id: TFT_LEGACY_VARIANT\n",
      "[1,1]<stderr>:    }\n",
      "[1,1]<stderr>:  }\n",
      "[1,1]<stderr>:}\n",
      "[1,1]<stderr>:\n",
      "[1,1]<stderr>:\twhile inferring type of node 'mean_squared_error/cond/output/_11'\n",
      "  6/191 [..............................] - ETA: 2s - loss: 13.6433   [1,0]<stderr>:/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "[1,0]<stderr>:  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "[1,0]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0094s vs `on_train_batch_end` time: 0.1490s). Check your callbacks.\n",
      "[1,1]<stderr>:/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "[1,1]<stderr>:  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "[1,1]<stderr>:WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0093s vs `on_train_batch_end` time: 0.1489s). Check your callbacks.\n",
      "191/191 [==============================] - 8s 12ms/step - loss: 3.3301<stdout>[1,0]<stdout[1,0]<stdout[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>[1,0]<stdout>\n"
     ]
    }
   ],
   "source": [
    "!horovodrun -np {GPU_COUNT} python tf_trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d97fe2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We demonstrated how to train a TensorFlow Keras model with the Merlin dataloader on multiple GPUs using Horovod.\n",
    "\n",
    "# Next Steps\n",
    "\n",
    "Merlin dataloader is part of NVIDIA Merlin, a open source framework for recommender systems. In this example, we looked only on a specific use-case to accelerate existing training pipelines. We provide more libraries to make recommender system pipelines easier and faster to work with:\n",
    "\n",
    "* [NVTabular](https://github.com/NVIDIA-Merlin/NVTabular) is a library to accelerate and scale feature engineering\n",
    "* [Merlin Models](https://github.com/NVIDIA-Merlin/models) is a library with high-quality implementations of popular recommender systems architectures\n",
    "\n",
    "The libraries are designed to work closely together. We recommend to check out our examples:\n",
    "\n",
    "* [Getting Started with NVTabular: Process Tabular Data On GPU](https://github.com/NVIDIA-Merlin/NVTabular/blob/stable/examples/01-Getting-started.ipynb)\n",
    "* [Getting Started with Merlin Models: Develop a Model for MovieLens](https://github.com/NVIDIA-Merlin/models/blob/stable/examples/01-Getting-started.ipynb)\n",
    "\n",
    "In the example, [From ETL to Training RecSys models - NVTabular and Merlin Models integrated example](https://github.com/NVIDIA-Merlin/models/blob/stable/examples/02-Merlin-Models-and-NVTabular-integration.ipynb), we explain how the close collaboration works."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
