{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb28e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77464844",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Getting Started with `Merlin dataloader`\n",
    "\n",
    "This notebook is created using the latest stable [merlin-pytorch](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-pytorch) container.\n",
    "\n",
    "## Overview\n",
    "\n",
    "[Merlin dataloader](https://github.com/NVIDIA-Merlin/dataloader) is a library for constructing highly optimized dataloaders to feed `Tensorflow/Keras` and `PyTorch` models during training. You preprocess your data using a [Merlin NVTabular](https://github.com/NVIDIA-Merlin/NVTabular) workflow and hand the dataset over to `Merlin dataloader`.\n",
    "\n",
    "In this notebook we will download the Movielens dataset consisting of movie ratings. We will briefly process the data using NVTabular and output it to a `Merlin Dataset`. Subsequently, we will construct a dataloader and build a simple `MatrixFactorization` model in vanilla PyTorch.\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "- Learn how `Merlin dataloader` integrates with `NVTabular` (a library for preprocessing tabular data on the GPU)\n",
    "- Understand `Merlin dataloader` high-level concepts\n",
    "- Use `Merlin dataloader` to train a `PyTorch` model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5598ae",
   "metadata": {},
   "source": [
    "# Downloading the dataset\n",
    "\n",
    "### MovieLens25M\n",
    "\n",
    "The [MovieLens25M](https://grouplens.org/datasets/movielens/25m/) is a popular dataset for recommender systems and is widely used in academic publications. The dataset contains 25M movie ratings for 62,000 movies given by 162,000 users. Many projects use only the user/item/rating information of MovieLens, but the original dataset provides metadata for the movies, as well. For example, which genres a movie has.\n",
    "\n",
    "In this notebook, we will only use the user-movie pairs along with the ratings a user assigned to the movie.\n",
    "\n",
    "Let's begin by downloading the [`MovieLens 25M Dataset`](https://grouplens.org/datasets/movielens/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d257d36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-11 05:41:26--  https://files.grouplens.org/datasets/movielens/ml-25m.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 261978986 (250M) [application/zip]\n",
      "Saving to: ‘ml-25m.zip.1’\n",
      "\n",
      "ml-25m.zip.1          5%[>                   ]  12.91M  3.06MB/s    eta 1m 53s ^C\n"
     ]
    }
   ],
   "source": [
    "!wget https://files.grouplens.org/datasets/movielens/ml-25m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3446b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!apt update\n",
    "!apt install unzip\n",
    "!unzip -q ml-25m.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c29f121",
   "metadata": {},
   "source": [
    "We have now downloaded and extracted the data and can read in the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e80cb9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.core.dispatch import get_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65e5ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = get_lib().read_csv('ml-25m/ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daffce3a",
   "metadata": {},
   "source": [
    "The `ratings.csv` file stores ratings a user has given a movie. Let's process the data, pass the resultant NVTabular dataset to `Merlin dataloader` and train a simple MatrixFactorization model that we will construct in `PyTorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10daa5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147880044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>306</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1147868817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147868828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>665</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147878820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>899</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1147868510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1      296     5.0  1147880044\n",
       "1       1      306     3.5  1147868817\n",
       "2       1      307     5.0  1147868828\n",
       "3       1      665     5.0  1147878820\n",
       "4       1      899     3.5  1147868510"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538f4654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/frame.py:384: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from nvtabular import *\n",
    "from nvtabular import ops\n",
    "\n",
    "dataset = Dataset(ratings)\n",
    "\n",
    "user_and_movie_ids = ['userId', 'movieId'] >> ops.Categorify(freq_threshold=100)\n",
    "workflow = Workflow(user_and_movie_ids + 'rating')\n",
    "\n",
    "processed = workflow.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d53c8a",
   "metadata": {},
   "source": [
    "We processed our `Merlin Dataset` using NVTabulars and performed the operations on the GPU. If you would like to learn more about the NVTabular library, please take a look [here](https://github.com/NVIDIA-Merlin/NVTabular).\n",
    "\n",
    "Now that we have preprocessed our data, let's instantiate the `dataloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34f1b32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from merlin.loader.torch import Loader\n",
    "loader = Loader(processed, batch_size=65536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc8b31ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824b56e2",
   "metadata": {},
   "source": [
    "From the `loader` we obtain a batch of data, a dictionary of tensors that have already been moved to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7794881b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'userId': tensor([[    0],\n",
       "          [    0],\n",
       "          [    0],\n",
       "          ...,\n",
       "          [28656],\n",
       "          [28656],\n",
       "          [28656]], device='cuda:0'),\n",
       "  'movieId': tensor([[   3],\n",
       "          [ 880],\n",
       "          [ 943],\n",
       "          ...,\n",
       "          [3504],\n",
       "          [   7],\n",
       "          [1226]], device='cuda:0'),\n",
       "  'rating': tensor([5.0000, 3.5000, 5.0000,  ..., 3.0000, 2.0000, 2.0000], device='cuda:0',\n",
       "         dtype=torch.float64)},\n",
       " None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af193bd9",
   "metadata": {},
   "source": [
    "Let us now construct a simple MatrixFactorization model and train for a single epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27f53605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89d9a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProduct(nn.Module):\n",
    "    def __init__(self, n_factors):\n",
    "        super().__init__()\n",
    "        self.user_embeddings = nn.Embedding(processed.schema['userId'].properties['domain']['max'], n_factors)\n",
    "        self.movie_embeddings = nn.Embedding(processed.schema['movieId'].properties['domain']['max'], n_factors)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        user_embs = self.user_embeddings(batch[0]['userId'])\n",
    "        movie_embs = self.movie_embeddings(batch[0]['movieId'])\n",
    "        \n",
    "        return (user_embs.squeeze(1) * movie_embs.squeeze(1)).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "744546f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d3736f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-2\n",
    "optim=Adam\n",
    "weight_decay=0\n",
    "\n",
    "model = DotProduct(64).cuda()\n",
    "optimizer = optim(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9bf5c7",
   "metadata": {},
   "source": [
    "Let's first calculate the Mean Squared Error loss before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1350e895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch_size = batch[0]['rating'].shape[0]\n",
    "            loss += criterion(model(batch), batch[0]['rating']) * batch_size\n",
    "            n += batch_size\n",
    "    return loss.item() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79310b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.42800231738009"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_loss(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa759a",
   "metadata": {},
   "source": [
    "Let us now train for a single epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8219c494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 740 ms, sys: 83.3 ms, total: 823 ms\n",
      "Wall time: 829 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.train()\n",
    "for batch in loader:\n",
    "    loss = criterion(model(batch), batch[0]['rating'].float())\n",
    "\n",
    "    # compute gradient and do an update step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d54d9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.262424255887305"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_loss(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d97fe2",
   "metadata": {},
   "source": [
    "The model has improved and has run for a single epoch, training on all 25_000_000 datapoints, in record time!\n",
    "\n",
    "In a more realistic scenario we would have singled out a validation dataset, trained for more epochs, optimized hyperparameters and so on.\n",
    "\n",
    "Nonetheless, the objective here was to quickly show you the ropes on integrating `Merlin dataloader` with `PyTorch`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "merlin": {
   "containers": [
    "nvcr.io/nvidia/merlin/merlin-tensorflow:latest"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
